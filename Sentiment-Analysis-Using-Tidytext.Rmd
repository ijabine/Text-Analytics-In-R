---
title: "Sentiment Analysis using tidytext package"
author: "Illarion Jabine"
date: "18/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


# Wrangling Text 
Since text is unstructured data, a certain amount of wrangling is required to get it into a form where you can analyze it. In this chapter, you will learn how to add structure to text by tokenizing, cleaning, and treating text as categorical data.
```{r}
# Load the tidyverse packages
library(tidyverse)
twitter_data <- readRDS("D:/R Projects/Text Analytics/twitter_data.rds")
# Print twitter_data
print(twitter_data)

# Print just the complaints in twitter_data
twitter_data %>% 
  filter(complaint_label == "Complaint")


# Start with the data frame
twitter_data %>% 
  # Group the data by whether or not the tweet is a complaint
  group_by(complaint_label) %>% 
  # Compute the mean, min, and max follower counts
  summarize(
    avg_followers = mean(usr_followers_count),
    min_followers = min(usr_followers_count),
    max_followers = max(usr_followers_count)
  )



```

Counts are the essential summary for categorical data. Since text is categorical, it's important to get comfortable computing counts. The twitter_data is composed of complaints and non-complaints, as indicated by the complaint_label column, and also includes a column indicating whether or not the user is verified (i.e., they have been confirmed by Twitter to be who they say they are) called usr_verified. Note that column is of type <lgl>, meaning logical. Do verified users complain more?

```{r}
twitter_data %>% 
  # Filter for just the complaints
  filter(complaint_label == "Complaint") %>% 
  # Count the number of verified and non-verified users
  count(usr_verified)


```


Summarizing user types


Since you can use the count() wrapper, why bother counting rows in a group as part of a grouped summary? Sometimes you want a more detailed summary, and knowing how to compute a count as part of a grouped summary that mixes numeric and categorical summaries can come in handy.

```{r}
twitter_data %>% 
  # Group by whether or not a user is verified
  group_by(usr_verified) %>% 
  summarize(
    # Compute the average number of followers
    avg_followers = mean(usr_followers_count),
    # Count the number of users in each category
    n = n()
  )

library(tidyverse)
library(tidytext)

tidy_twitter <- twitter_data %>% 
  # Tokenize the twitter data
  unnest_tokens(word, tweet_text) 

tidy_twitter %>% 
  # Compute word counts
  count(word) %>% 
  # Arrange the counts in descending order
  arrange(desc(n))


```


Cleaning and counting


Remove stop words to explore the content of just the airline tweets classified as complaints in twitter_data.
•Tokenize the tweets in twitter_data. Name the column with tokenized words as word. 
•Remove the default stop words from the tokenized twitter_data.
•Filter to keep the complaints only.
•Compute word counts using the tokenized, cleaned text and arrange in descending order by count.

```{r}
tidy_twitter <- twitter_data %>% 
  # Tokenize the twitter data
  unnest_tokens(word, tweet_text) %>% 
  # Remove stop words
  anti_join(stop_words)

tidy_twitter %>% 
  # Filter to keep complaints only
  filter(complaint_label == "Complaint") %>% 
  # Compute word counts and arrange in descending order
  count(word) %>% 
  arrange(desc(n))
```
It looks like complaints include frequent references to time, delays, and service. However, there are simply a lot of specific airlines referenced. These could be considered as stop words specific to this data, and we'll see how to remove them in the next chapter.


# Visualizing Text

While counts are nice, visualizations are better. In this chapter, you will learn how to apply what you know from ggplot2 to tidy text data.

Visualizing complaints


We ended the last chapter with complaint word counts. Now let's visualize those word counts with a bar plot.

The tidyverse and tidytext packages have been loaded. twitter_data has been tokenized and the standard stop words have been removed.

```{r}
word_counts <- tidy_twitter %>% 
  filter(complaint_label == "Complaint") %>% 
  count(word) %>% 
  # Keep words with count greater than 100
  filter(n > 100)

# Create a bar plot using word_counts with x = word
ggplot(word_counts, aes(x = word, y = n)) +
  geom_col() +
  # Flip the plot coordinates
  coord_flip()
```


Visualizing non-complaints


Now let's visualize the word counts associated with non-complaints.
```{r}
word_counts <- tidy_twitter %>% 
  # Only keep the non-complaints
  filter(complaint_label == "Non-Complaint") %>% 
  count(word) %>% 
  filter(n > 150)

# Create a bar plot using the new word_counts
ggplot(word_counts, aes(x = word, y = n)) +
  geom_col() +
  coord_flip() +
  # Title the plot "Non-Complaint Word Counts"
  ggtitle("Non-Complaint Word Counts")

```

We still have some terms that look like stop words specific to this application. Also, it would be nice to plot both of these side-by-side!


Adding custom stop words


We've seen a number of words in twitter_data that aren't informative and should be removed from your final list of words. In this exercise, you will add a few words to your custom_stop_words data frame .
```{r}
custom_stop_words <- tribble(
  # Column names should match stop_words
  ~word, ~lexicon,
  # Add http, win, and t.co as custom stop words
  "http", "CUSTOM",
  "win", "CUSTOM",
  "t.co", "CUSTOM"
)

# Bind the custom stop words to stop_words
stop_words2 <- stop_words %>% 
  bind_rows(custom_stop_words)
```

Visualizing word counts using factors


I've added a number of other custom stop words (including the airline names) and tidied the data for you. Now you will create an improved visualization and plot the words arranged in descending order by word count.
```{r}
word_counts <- tidy_twitter %>% 
  filter(complaint_label == "Non-Complaint") %>% 
  count(word) %>% 
  # Keep terms that occur more than 100 times
  filter(n > 100) %>% 
  # Reorder word as an ordered factor by word counts
  mutate(word2 = fct_reorder(word, n))

# Plot the new word column with type factor
ggplot(word_counts, aes(x = word2, y = n)) +
  geom_col() +
  coord_flip() +
  ggtitle("Non-Complaint Word Counts")
```


Counting by product and reordering


tidy_twitter has been tokenized and stop words, including custom stop words, have been removed. You would like to visualize the differences in word counts based on complaints and non-complaints.

```{r}
word_counts <- tidy_twitter %>%
  # Count words by whether or not its a complaint
  count(word, complaint_label) %>%
  # Group by whether or not its a complaint
  group_by(complaint_label) %>%
  # Keep the top 20 words
  top_n(20, n) %>%
  # Ungroup before reordering word as a factor by the count
  ungroup() %>%
  mutate(word2 = fct_reorder(word, n))
```


Visualizing word counts with facets


The word_counts from the previous exercise have been loaded. Let's visualize the word counts for the Twitter data with separate facets for complaints and non-complaints.

```{r}
# Include a color aesthetic tied to whether or not its a complaint
ggplot(word_counts, aes(x = word2, y = n, fill = complaint_label)) +
  # Don't include the lengend for the column plot
  geom_col(show.legend = FALSE) +
  # Facet by whether or not its a complaint and make the y-axis free
  facet_wrap(~ complaint_label, scales = "free_y") +
  # Flip the coordinates and add a title: "Twitter Word Counts"
  coord_flip() +
  ggtitle("Twitter Word Counts")
```


Creating a word cloud


We've seen bar plots, now let's visualize word counts with word clouds! tidy_twitter has already been loaded, tokenized, and cleaned.

```{r}
# Load the wordcloud package
library(wordcloud)

# Compute word counts and assign to word_counts
word_counts <- tidy_twitter %>% 
  count(word)

wordcloud(
  # Assign the word column to words
  words = word_counts$word, 
  # Assign the count column to freq
  freq = word_counts$n,
  max.words = 30
)
```

Adding a splash of color


What about just the complaints? And let's add some color. Red seems appropriate. The wordcloud package has been loaded along with tidy_twitter

```{r}
# Compute complaint word counts and assign to word_counts
word_counts <- tidy_twitter %>% 
  filter(complaint_label == "Complaint") %>% 
  count(word)

# Create a complaint word cloud of the top 50 terms, colored red
wordcloud(
  words = word_counts$word, 
  freq = word_counts$n,
  max.words = 50, 
  colors = "red"
)
```



# Sentiment Analysis

While word counts and visualizations suggest something about the content, we can do more. In this chapter, we move beyond word counts alone to analyze the sentiment or emotional valence of text.

Counting the NRC sentiments


The fourth dictionary included with the tidytext package is the nrc dictionary. Let's start our exploration with sentiment counts.

```{r}
# Count the number of words associated with each sentiment in nrc
library(textdata)
get_sentiments("nrc") %>% 
  count(sentiment) %>% 
  # Arrange the counts in descending order
  arrange(desc(n))
```


Visualizing the NRC sentiments


We've seen how visualizations can give us a better idea of patterns in data than counts alone. Let's visualize the sentiments from the nrc dictionary. I've loaded the tidyverse and tidytext packages for you already.

```{r}
# Pull in the nrc dictionary, count the sentiments and reorder them by count
sentiment_counts <- get_sentiments("nrc") %>% 
  count(sentiment) %>% 
  mutate(sentiment2 = fct_reorder(sentiment, n))

# Visualize sentiment_counts using the new sentiment factor column
ggplot(sentiment_counts, aes(x = sentiment2, y = n)) +
  geom_col() +
  coord_flip() +
  # Change the title to "Sentiment Counts in NRC", x-axis to "Sentiment", and y-axis to "Counts"
  labs(
    x = "Sentiment",
    y = "Counts",
    title = "Sentiment Counts in NRC"
  )
```


Counting sentiment


The tidy_twitter dataset has been loaded for you. Let's see what sort of sentiments are most prevalent in our Twitter data.

```{r}
# Join tidy_twitter and the NRC sentiment dictionary
sentiment_twitter <- tidy_twitter %>% 
  inner_join(get_sentiments("nrc"))

# Count the sentiments in sentiment_twitter
sentiment_twitter %>% 
  count(sentiment) %>% 
  # Arrange the sentiment counts in descending order
  arrange(desc(n))
```

Visualizing sentiment


Let's explore which words are associated with each sentiment in our Twitter data.
•Inner join tidy_twitter to the NRC dictionary and filter for positive, fear, and trust.
•Count by word and sentiment and keep only the top 10 of each sentiment.
•Create a factor called word2 that has each word ordered by the count.

```{r}
word_counts <- tidy_twitter %>% 
  # Append the NRC dictionary and filter for positive, fear, and trust
  inner_join(get_sentiments("nrc")) %>% 
  filter(sentiment %in% c("positive", "fear", "trust")) %>%
  # Count by word and sentiment and take the top 10 of each
  count(word, sentiment) %>% 
  group_by(sentiment) %>% 
  top_n(10, n) %>% 
  ungroup() %>% 
  # Create a factor called word2 that has each word ordered by the count
  mutate(word2 = fct_reorder(word, n))
```


•Create a bar plot of the word counts colored by sentiment.
•Create a separate facet for each sentiment with free axes.
•Title the plot "Sentiment Word Counts" with "Words" for the x-axis.

```{r}
# Create a bar plot out of the word counts colored by sentiment
ggplot(word_counts, aes(x = word2, y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  # Create a separate facet for each sentiment with free axes
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip() +
  # Title the plot "Sentiment Word Counts" with "Words" for the x-axis
  labs(
    x ="Words",
    title ="Sentiment Word Counts"
  )
```
These word counts by sentiment illustrate a possible mismatch with this particular sentiment dictionary. For example, gate is listed under trust. Pay is listed under both trust and positive. Remember, our sentiment analysis is conditioned on the dictionary we use. It's a tall order, but finding or building a sentiment dictionary that is context-specific would be ideal. 



Practicing reshaping data

The spread() verb allows us to quickly reshape or stack and transpose our data, making it easier to mutate().


```{r}
tidy_twitter %>% 
  # Append the NRC sentiment dictionary
  inner_join(get_sentiments("nrc")) %>%  
  # Count by complaint label and sentiment
  count(complaint_label, sentiment) %>%  
  # Spread the sentiment and count columns
  spread(sentiment,n)
```
Each of the sentiments in NRC is its own column with the associated counts as values. 



Practicing with grouped summaries


We can use spread() in association with the output of grouped summaries as well.
•Append tidy_twitter to the afinn sentiment dictionary.
•Group by both complaint label and whether or not the user is verified.
•Summarize the data to create a new column, aggregate_value, which contains the sum of value.
•Spread the complaint_label and aggregate_value columns.

```{r}
tidy_twitter %>% 
  # Append the afinn sentiment dictionary
  inner_join(get_sentiments("afinn")) %>% 
  # Group by both complaint label and whether or not the user is verified
  group_by(complaint_label, usr_verified) %>% 
  # Summarize the data with an aggregate_value = sum(value)
  summarize(aggregate_value = sum(value)) %>% 
  # Spread the complaint_label and aggregate_value columns
  spread(complaint_label, aggregate_value) %>% 
  mutate(overall_sentiment = Complaint + `Non-Complaint`)
```
With the output of the grouped summary spread(), we can easily use mutate() to create a new overall_sentiment column. It looks like unverified users complain more often, on aggregate. 


Visualizing sentiment by complaint type


Now let's see whether or not complaints really are more negative, on average.
•Append tidy_twitter to the bing sentiment dictionary.
•Count by complaint label and sentiment.
•Spread the sentiment and count columns.
•Add a new column, overall_sentiment, as positive - negative.

```{r}
sentiment_twitter <- tidy_twitter %>% 
  # Append the bing sentiment dictionary
  inner_join(get_sentiments("bing")) %>% 
  # Count by complaint label and sentiment
  count(complaint_label, sentiment) %>% 
  # Spread the sentiment and count columns
  spread(sentiment, n) %>% 
  # Compute overall_sentiment = positive - negative
  mutate(overall_sentiment = positive - negative)

```
•Create a bar plot of overall sentiment by complaint label, colored by complaint level (as a factor). 
•Title the plot "Overall Sentiment by Complaint Type," with the subtitle "Airline Twitter Data".

```{r}
# Create a bar plot out of overall sentiment by complaint level, colored by a complaint label factor
ggplot(
  sentiment_twitter, 
  aes(x = complaint_label, y = overall_sentiment, fill = as.factor(complaint_label))
) +
  geom_col(show.legend = FALSE) +
  coord_flip() + 
  # Title the plot "Overall Sentiment by Complaint Type," with an "Airline Twitter Data" subtitle
  labs(
    title = "Overall Sentiment by Complaint Type",
    subtitle = "Airline Twitter Data"
  )
```

