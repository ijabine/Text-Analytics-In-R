---
title: "Sentiment Analysis Using tidytext Package"
author: "Illarion Jabine"
date: "18/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Required packages:

* [tidyverse]
* [tidytext]
* [textdata]
* [wordcloud]
* [tokenizers]

## Introduction


## 1. Loading raw text and some preliminary analysis. 
Text is unstructured data, and therefor a certain amount of text wrangling is required to get it into a form ready for analysis. We can add structure to text by tokenizing, cleaning, and treating text as categorical data.

Let's first load the libraries. In this exercise I will use the text mining package tidytext, although there are plenty of other text analytics packages available: tm, quanteda, text2vec, etc. The creators of tidytext package claim that this package follow the principles of the tidy data. See <https://en.wikipedia.org/wiki/Tidy_data>

```{r loading packages}
# Load the tidyverse packages
library(tidyverse)
library(tidytext)
library(textdata)
library(wordcloud)

```

Now we will read twitter data from RDS file:
```{r}
twitter_data <- readRDS("data/twitter_data.rds")

# Print twitter_data
print(twitter_data)

str(twitter_data)


```

I can assume that this data were harvested (scrapped) from twitter using twitteR package. The twitter_data is composed of tweet text stored in the column tweet_text. The purpose of this dataset is to preform the sentiment analysis. 
Each tweet is labeled with  complaints and non-complaints, as indicated by the complaint_label column, and also includes columns indicating whether or not the user is verified called "usr_verified" and the count of user followers (usr_followers_count). 
Since text is categorical, counts are the essential summary for categorical data. 
By using already available data in complaint_label and usr_verified variables we can perform some discriptive analysis: 
```{r some descriptive analysis}
twitter_data %>% 
  # Count the number of verified/non-verified users and complaint/non-complaint
  count(complaint_label,usr_verified) %>%
  ggplot(aes(x= complaint_label,y=n,fill=usr_verified)) +
  geom_col(position = "stack") +
  labs(x = "Complaint: Y / N",
       y = "Count") +
  # Legend title and labels can be controlled with scale_fill_discrete():
  scale_fill_discrete(name="User Verified:\n No or Yes",
                      labels=c("No", "Yes"))


```


## 2. Text Preprocessing

We have prepared our input datasets with raw textual data.
Now we are ready to transform it into the format required for text analytics. This transformation process has several steps:
 1. Tokenization
 2. Creation of DFM: document-frequency matrix (aka document-term matrix)

### 2.1 Tokenization

Tokenization is a process of converting natural language text into tokens (or terms). Tokens can be shingle n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets.
To tokenize I will use unnest_tokens() function from tidytext package. 
This function  uses the tokenizers package, splitting the table into one-token-per-row, i.e. one row per word:

```{r tokenization}

twitter_dfm <- twitter_data %>% 
  # Tokenize the twitter data
  unnest_tokens(word, tweet_text) 

twitter_dfm %>% 
  # Compute word counts
  count(word) %>% 
  # Arrange the counts in descending order
  arrange(desc(n))

```

It looks like unnest_tokens() has also done some text pre-processing. Compare the first tweet as a raw text and in tokenized format:
```{r}
print(twitter_data$tweet_text[1])
view(twitter_dfm[1:25,])
```
It has made it lower case, removed punctuation, symbols and tags. However, I did not ask it to do this. 
It seems to be controled by tokenize_words() function from tokenizers package:
tokenize_words(x, lowercase = TRUE, stopwords = NULL, strip_punct = TRUE,
  strip_numeric = FALSE, simplify = FALSE)
If I used token() from quanteda package I could specify all these parameters:

twitter_dfm_quanteda <- tokens(twitter_data$tweet_text, what = "word", 
     remove_numbers = TRUE, 
     remove_punct = TRUE, 
     remove_symbols = TRUE, 
     remove_hyphens = TRUE)

### 3. Removing stop words

The tokenized data still contain a lot of stop words (a, the, to, after,...) that do not add more meaning. 
To remove them I will join twitter_data with stop_words lexicon from tidytext using anti_join from dplyr package.

Note: anti_join(x,y) - return all rows from x where there are not matching values in y, keeping just columns from x.


```{r removing stop words}
twitter_dfm <- twitter_data %>% 
  # Tokenize the twitter data
  unnest_tokens(word, tweet_text) %>% 
  # Remove stop words
  anti_join(stop_words)
  
```


### 4. Visualizing Text

Combining the power of dplyr and ggplot2 we can create various visualizations.

 4.1 Non-complaints arranged in descending order by word count:
```{r visualize the word counts associated with non-complaints}
twitter_dfm %>% 
  filter(complaint_label == "Non-Complaint") %>% 
  count(word) %>% 
  # Keep terms that occur more than 170 times
  filter(n > 170) %>% 
  # Reorder word as an ordered factor by word counts
  mutate(word2 = fct_reorder(word, n)) %>%
# Plot the new word column with type factor
ggplot(aes(x = word2, y = n)) +
  geom_col() +
  coord_flip() +
  labs(title="Non-Complaint Word Counts",
       x = "Words",
       y = "Count")
```

It looks like non-complaints include frequent references to specific airlines. time, delays, and service. These could be considered as stop words specific to this data, and can be removed using custom stop words.

 4.2 Visualizing word counts with facets:
```{r facets}
twitter_dfm %>%
  # Count words by whether or not its a complaint
  count(word, complaint_label) %>%
  # Group by whether or not its a complaint
  group_by(complaint_label) %>%
  # Keep the top 20 words, top_n() -convenient wrapper that uses filter() and min_rank() to select the top or bottom entries. 
  top_n(20, n) %>%
  # Ungroup before reordering word as a factor by the count
  ungroup() %>%
  mutate(word2 = fct_reorder(word, n)) %>%
  # Sending the result of these dplyr transformations to ggplot:
  ggplot(aes(x = word2, y = n, fill = complaint_label)) +
  # Don't include the lengend for the column plot
  geom_col(show.legend = FALSE) +
  # Facet by whether or not its a complaint and make the y-axis free
  facet_wrap(~ complaint_label, scales = "free_y") +
  # Flip the coordinates and add a title: "Twitter Word Counts"
  coord_flip() +
  labs(title="Word Counts",
       x = "Words",
       y = "Count")
```

 4.3 Creating a word cloud

Let's visualize word counts with word clouds. For that we will need to use wordcloud() from wordcloud package: 
```{r word cloud}

# First compute word counts and assign to word_counts
word_counts <- twitter_dfm %>% 
  count(word)

wordcloud(
  # Assign the word column to words
  words = word_counts$word, 
  # Assign the count column to freq
  freq = word_counts$n,
  max.words = 30,
  colors = "red"
)

```


### 5. Adding custom stop words

A number of words in twitter_data are not informative and should be removed from DFM. Let'sl add a few words to custom_stop_words:

```{r custom stop words}
custom_stop_words <- tribble(
  # Column names should match stop_words
  ~word, ~lexicon,
  # Add http, win, and t.co as custom stop words
  "http", "CUSTOM",
  "delta", "CUSTOM",
  "united", "CUSTOM",
  "klm", "CUSTOM",
  "ryanair", "CUSTOM",
  "jetblue", "CUSTOM"
)
# I use tribble() to create a tibble. 
# tibble provides stricter checking and better formatting than the traditional data fram <http://tibble.tidyverse.org>

# Bind the custom stop words to stop_words (comes seeded with tidytext), I use bind_rows() from dplyr:
stop_words2 <- bind_rows(custom_stop_words,stop_words)

# Now we can use our custom stop_words2 list to remove them from twitter_data:

twitter_dfm <- twitter_data %>% 
  # Tokenize the twitter data
  unnest_tokens(word, tweet_text) %>% 
  # Remove stop words
  anti_join(stop_words2)

```

### 6. Sentiment Analysis

We can go further and move beyond word counts (content of the text) to analyze the sentiment or emotional intent of words. 
By performing sentiment analysis (or opinion mining)  we can infer whether a section of text is positive or negative. 
Likewise, we can characterize text along other emothions like anger, joy, disgust.

 6.1 Sentiment lexicons
There are four dictionaries or sentiment lexicons included with the tidytext package. All of these lexicons are based on single words or unigrams:

 * afinn: assigns scores from -5 to 5, with negative scores indicating negative sentiment and positive scores - positive sentiment
 * bing: binary categories of positive and negative
 * loughran: categorize words into constraining,litigious,negative,positive,superfluous,  uncertainty
 * nrc: categorize words into positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.
Finding or building a sentiment dictionary that is context-specific would be ideal.

Let's pull in the sentiment dictionaries, save them as objects and visualize their content:
```{r loading lexicons, message=FALSE, warning=FALSE}

afinn_lex <- get_sentiments("afinn")
bing_lex <- get_sentiments("bing")
loughran_lex <- get_sentiments("loughran")
nrc_lex <- get_sentiments("nrc")

#Visualizing the loughran sentiments:
loughran_lex %>%
  count(sentiment) %>% 
  mutate(sentiment2 = fct_reorder(sentiment, n)) %>%
  # Visualize sentiment_counts using the new sentiment factor column
ggplot(aes(x = sentiment2, y = n)) +
  geom_col() +
  coord_flip() +
  # Change the title to "Sentiment Counts in NRC", x-axis to "Sentiment", and y-axis to "Counts"
  labs(
    x = "Sentiment",
    y = "Counts",
    title = "Sentiment Counts in loughran"
  )

#Visualizing the NRC sentiments:
nrc_lex %>%
  count(sentiment) %>% 
  mutate(sentiment2 = fct_reorder(sentiment, n)) %>%
  # Visualize sentiment_counts using the new sentiment factor column
ggplot(aes(x = sentiment2, y = n)) +
  geom_col() +
  coord_flip() +
  # Change the title to "Sentiment Counts in NRC", x-axis to "Sentiment", and y-axis to "Counts"
  labs(
    x = "Sentiment",
    y = "Counts",
    title = "Sentiment Counts in NRC"
  )
  
```

 6.2 Analyzing sentiments


Now when we have all lexicons at our disposal we can do sentiment analysis.
Let's see what sort of sentiments are most prevalent in the harvested twitts.
We will do it by joining the twitter_dfm dataset with lexicon dataset using inner_join
and word column as a key.

```{r sentiment analysis}
# Join twitter_dfm and the loughran sentiment dictionary using inner_join:
twitter_sentiment_loughran <- inner_join(twitter_dfm,loughran_lex,by = "word")

# Join twitter_dfm and the NRC sentiment dictionary using inner_join:
twitter_sentiment_nrc <- inner_join(twitter_dfm,nrc_lex,by = "word")

# Join twitter_dfm and the afinn sentiment dictionary using inner_join:
twitter_sentiment_affin <- inner_join(twitter_dfm,afinn_lex,by = "word")

# Join twitter_dfm and the bing sentiment dictionary using inner_join:
twitter_sentiment_bing <- inner_join(twitter_dfm,bing_lex,by = "word")

# Count and compare the sentiments in twitter_sentiment_nrc and twitter_sentiment_loughran

twitter_sentiment_nrc %>% 
  count(sentiment) %>% 
  inner_join(twitter_sentiment_loughran %>% 
  count(sentiment),by="sentiment")

```


 6.3 Visualizing sentiment

Let's explore which words are associated with anger, fear, sadness and disgust sentiments in twitter_sentiment_nrc.

```{r}
twitter_sentiment_nrc  %>% 
  # Filter for anger, fear, sadness and disgust
  filter(sentiment %in% c("anger", "fear", "sadness", "disgust")) %>%
  # Count by word and sentiment and take the top 20 of each
  count(word, sentiment) %>% 
  group_by(sentiment) %>% 
  top_n(20, n) %>% 
  ungroup() %>% 
  # Create a factor called word2 that has each word ordered by the count
  mutate(word2 = fct_reorder(word, n)) %>%
  # Create a bar plot out of the word counts colored by sentiment
ggplot(aes(x = word2, y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  # Create a separate facet for each sentiment with free axes
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip() +
  labs(
    x ="Words",
    title ="Sentiment Word Counts"
  )

```

These word counts by sentiment show a possible mismatch with this particular sentiment dictionary. The words are listed under different categories. Our sentiment analysis is conditioned on the dictionary. It would be ideal to find or build a sentiment dictionary that is context-specific. 


 6.4 Computing overall sentiment with afinn lexicon dictionary
 
Remember that afinn lexicon assigns scores from -5 to 5, with negative scores indicating negative sentiment and positive scores - positive sentiment

```{r}
twitter_sentiment_affin %>%
  # Group by both complaint label and whether or not the user is verified
  group_by(complaint_label, usr_verified) %>% 
  # Summarize to create a new column "aggregate_value" which contains the sum of value
  summarize(aggregate_value = sum(value)) %>% 
  # Spread the complaint_label and aggregate_value columns
  # spread() allows us to quickly reshape or stack and transpose the data, making it easier to mutate.
  # Spread the complaint_label and aggregate_value columns
  spread(complaint_label, aggregate_value) %>% 
  mutate(overall_sentiment = Complaint + `Non-Complaint`)
  
```

Overall Sentiment by Complaint Type

```{r}
twitter_sentiment_bing %>% 
  # Count by complaint label and sentiment
  count(complaint_label, sentiment) %>% 
  # Spread the sentiment and count columns
  spread(sentiment, n) %>% 
  # Compute overall_sentiment = positive - negative by adding a new column overall_sentiment
  mutate(overall_sentiment = positive - negative) %>%
  ggplot(
  aes(x = complaint_label, y = overall_sentiment, fill = as.factor(complaint_label))
) +
  geom_col(show.legend = FALSE) +
  coord_flip() + 
  # Title the plot "Overall Sentiment by Complaint Type," with an "Airline Twitter Data" subtitle
  labs(
    title = "Overall Sentiment by Complaint Type",
    subtitle = "Airline Twitter Data"
  )

```
