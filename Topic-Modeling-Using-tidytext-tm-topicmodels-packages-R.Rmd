---
title: "Topic modeling using tidytext,tm and topicmodels packages"
author: "Illarion Jabine"
date: "27/12/2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

### Required packages:

* [tidyverse]
* [tidytext]: text analysis toolbox
* [tm]: Comprehensive text mining package
* [topicmodels]

### Key terms
 * Latent Dirichlet allocation (LDA)
 * tidytext integration with other NLP R packages
 * document-term matrix (DTM)
 * document-frequency matrix (DFM)
 
## Introduction

Topic modeling is an unsupervised method of clustering documents into destinct groups or topics.
In Latent Dirichlet allocation (LDA) each document is represented as a mixture of topics, whereas each topic is a mixture of words (terms). 
Some terms may belong to several topics simultaneously creating overlaps.
In two topic model, LDA can identify a document as 80% topic A and 20% topic B.
LDA can both identify the group of words that describe each topic, and determin the mixture of topics in each document. 

### Useful Links
 * For a comprehensive list of R NLP packages see <https://cran.r-project.org/web/views/NaturalLanguageProcessing.html>.
 * For Regular Expressions (regex): <https://www.rexegg.com/> 

### Integration of tidytext package with other NLP R packages

tidytext package has functionality that allows it to read and save objects created in popular NLP packages "tm" and "quanteda".
The text mining packages work with a document-term matrix (DTM) in tm package
and document-frequency matrix (DFM) in quanteda package.
DTM/DFM is constructed in the following way:
 * each row represents one document,
 * each column represents one term,
 * each cell contains the frequency of appearances of that term in the document.
1. tidy() function turns a DTM/DFM into a tidy one-term-per-row data frame.
2. cast_dtm() converts to a DocumentTermMatrix object from tm
3. cast_dfm() converts to a dfm object from quanteda
4. cast_sparse() converts to a sparse matrix from the Matrix package


### Load the libraries
Let's first load the libraries. 
```{r loading packages, message=FALSE, warning=FALSE}

library(tidyverse)
library(tidytext)
library(tm)
library(topicmodels)


```


### 1. Loading text files
We are going to work with 6 short stories by a Russian writer Anton Chekhov. 
```{r Loading text files, message=FALSE, warning=FALSE}
# We need to setup the current directory and point to where the source text files are located.
setwd("../Text-Analytics-In-R/data/topic_model")
# Read the contents of the directory and create a file name vector 
file_names <- list.files(getwd(),pattern = "*.txt")
# saving file names without .txt
doc_names <- str_split_fixed(file_names,".txt",2)[,1]

# removing _ and extra blanks
doc_names <- doc_names %>% str_replace_all("_"," ") %>% str_replace_all("  "," ")

# Read the contents of files and create a list of documents
raw_text <- lapply(file_names, readLines)

# assign document names to the elements of the list
names(raw_text) <- doc_names
```

### 2. Creating a corpus of documents
Text corpus (plural corpora) is a large and structured set of texts or collections of documents containing (natural language) text. 
It is used  to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules.
A Corpus is the main structure for managing documents in tm (text mining) package. 
Corpus() function creates a Corpus object with conent of each document and additinal metadata.
Metadata is used to annotate text documents or whole corpora with additional information.
Use meta() function to update metadate.
```{r creating corpus, message=FALSE, warning=FALSE}
doc_corpus <- Corpus(VectorSource(raw_text))
```

### 3. Corpus text pre-processing 
tm package offers a comprehensive tm_map function that allows to apply transformation functions (also denoted as mappings) to corpora. The transformations can include:
 1. removal of numbers, punctuations
 2. stopwords
 3. stripping of white spaces
 4. stemming
 5. etc...
```{r  corpus pre-processing, message=FALSE, warning=FALSE}
# remove stop words
doc_corpus_clean <- tm_map(doc_corpus, removeWords, stopwords("english"))

# define my own stopword list
custom_stopwords <- c("he","him","her","she","her","me","my","I","they","them","the","a")
# remove custom stop words
doc_corpus_clean <- tm_map(doc_corpus_clean, removeWords, custom_stopwords)

# remove numbers from corpus
doc_corpus_clean <- tm_map(doc_corpus_clean, removeNumbers)

# remove punctuation from document corpus
doc_corpus_clean <- tm_map(doc_corpus_clean, removePunctuation)

# Eliminating extra whitespac
doc_corpus_clean <- tm_map(doc_corpus_clean, stripWhitespace)

#Convert to lower case
doc_corpus_clean <- tm_map(doc_corpus_clean, content_transformer(tolower))

# Stemming
doc_corpus_clean <- tm_map(doc_corpus_clean, stemDocument)

```



### 4. Document Term/Term Document Matrix
A common approach in text mining is to create a term-document matrix from a corpus.
TermDocumentMatrix() and DocumentTermMatrix() (depending on whether you want terms as rows and
documents as columns, or vice versa) are used to create DTM.
```{r Document Term/Term Document Matrix, message=FALSE, warning=FALSE}
# define control parameters to create a DTM. 
control_params <- list(stopwords = TRUE,weighting = weightTf,
                   stemming = TRUE,removeNumbers = TRUE,
                   minDocFreq = 1)
# creat DTM (documents in rows)
dtm <- DocumentTermMatrix(doc_corpus_clean, control = control_params)

inspect(removeSparseTerms(dtm, 0.4))

# create TDM (terms in rows)
tdm <- TermDocumentMatrix(doc_corpus_clean, control = control_params)
inspect(removeSparseTerms(tdm, 0.4))
```

### 5. Latent (i.e. hidden) Dirichlet allocation

Now when we have our DTM we can apply Latent Dirichlet allocation to it.
LDA is guided by 2 principles:
 1. Every document is a mixture of topics: 
 Each document may contain words from several topics in particular proportions. 
 For example in 2 topic model, Document1 is 80% topic A and 20% topic B. Document 2 is viceversa.
 2. Every topic is a mixture of words:
 For example a two-topic model of a magazine, with one topic of "art" and the other of "travel".
 Art topic may consist of words "painting", "museum", whereas travel "sightseeing", and might also contain the word "museum". 
 It is important to understand that words can be shared among the topics.
Why latent or hidden is in the name of LDA?
Somehow we might know beforehand that several topics or themes exist in the corpus of documents.
These topics are not observed (they are just assumed), but we can see the documents and words.
So, we say that topics are hidden or latent.

I will use LDA() function from topicmodels package:
```{r LDA, message=FALSE, warning=FALSE}

# We can specify certain control parameters for LDA algorithm and for Gibbs sampling:
lda_control_params <- list(burnin = 3000, iter = 1000, 
                           thin = 200, seed = list(12345,2323,434,452,88), 
                           nstart = 5, best = TRUE)
# Number of topics, well we have 6 books, let's assume we have 6 topics :)
k <- 6

# Now we can run LDA with Gibbs sampling:
chekhov_lda_model_gibbs <- LDA(dtm, k, method= "Gibbs", control = lda_control_params)

# Or we can use default method = VEM:
chekhov_lda_model_vem <- LDA(dtm,k,control = list(seed = 1234))
```

The LDA model has been created. The model estimates two types of probabilities:
 1. "beta": per-topic-per-word probabilities - describes a distribution of words in a topic.
 This is word-topic probability distribution
 2. "gamma": per-document-per-topic probabilities - describes a distribution of topics in a document.
 This is document-topic probability distribution.
To extract these two probabilities from the LDA object, I will use tidy() function
```{r extracting beta, message=FALSE, warning=FALSE}
chekhov_topics <- tidy(chekhov_lda_model_vem, matrix = "beta")

```
chekhov_topics is a dataframe with one-topic-per-term-per-row format (so-called tidy format).
For each combination of topic and term, the model computes the probability of that term being generated from that topic.
 
 5.1 Topic identification
 
Now we can use tidyverse infrastructure to visualize the topics identified by LDA.
Below are histograms with the terms that are most common within each topic:

```{r visualizing topics, message=FALSE, warning=FALSE}
chekhov_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```

As I specifically asked the system to find 6 topics (k <- 6) by looking at each topic we can sort of
figure out the main topic idea. For example, the first topic is about Nadia's love to Gorni and her thoughts about that.
Let's check this by using tidy() on the document-term matrix and selecting the most common words in "AFTER THE THEATRE" short store, which is 2nd document.

```{r AFTER THE THEATRE word count}
tidy(dtm) %>%
  filter(document == 2) %>%
  arrange(desc(count))

```
It seems that the results of LDA and the word frequency count correlate with each other.

 5.2 Mapping documents to topics
Once we have identified topics within the documents we need to map documents to those topics.
In addition to estimating each topic as a mixture of words, LDA also models each document as a mixture of topics. 
To do that we need to examine the per-document-per-topic probabilities or “gamma”.
This can be done using tidy() to retrieve the statistical components of the model:
```{r extracting gamma, message=FALSE, warning=FALSE }
chekhov_topics_documents <- tidy(chekhov_lda_model_vem, matrix = "gamma")

```

```{r}
# Visualizing the mapping between documents and topics.
chekhov_topics_documents %>% top_n(6,gamma) %>% select(document,topic) %>% arrange(document)
chekhov_topics_documents %>% arrange(document)
```

For example, the LDA model estimates that about 99.99% of the words in document 1 originate from topic 6 (gamma is nearly 1).


Now let's apply LDA to 2 longer stories "The lady with the dog" and "Ward No 6" by Chekhov which are made up of 4 and 19 chapters respectevely.
We treat each chapter as a separate document.

```{r the lady with the dog, message=FALSE, warning=FALSE}
setwd("../Text Analytics New/data/topic_model")
raw_text_lady <- readLines("THE LADY WITH THE DOG.txt")
raw_text_ward <- readLines("WARD NO 6.txt")
# convert to a data frame
raw_text_lady <- as.data.frame(raw_text_lady,stringsAsFactors = FALSE)
raw_text_ward <- as.data.frame(raw_text_ward,stringsAsFactors = FALSE)
# change the column name to "text"
colnames(raw_text_lady) <- c("text")
colnames(raw_text_ward) <- c("text")

# create a new column with chapter number, and also assign document names,
# the document column is required later when DTM matrix is created with cast_dtm().
text_lady_by_chapter <- raw_text_lady %>%
  mutate(document = c("The lady with the dog"),
    chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE))))

text_ward_by_chapter <- raw_text_ward %>%
  mutate(document = c("Ward No 6"),
    chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE))))

# bind all together
text <- bind_rows(text_lady_by_chapter,text_ward_by_chapter)
# create one-term-per-row matrix using unnest_tokens() from tidytext package
text_by_chapter_word <- text %>% unnest_tokens(word,text)

# calculate word frequencies and remove stop words

text_word_count <- text_by_chapter_word %>% 
  anti_join(stop_words) %>%
  count(document,word, sort = TRUE) %>%
  ungroup()

# Now we have to convert lady_word_count (which is in tidy format) into DTM format understandable by LDA model. 
# To do that we have to use cast_dtm() from tidytext package

dtm <- text_word_count %>% cast_dtm(document,word,n)

# Now let's apply LDA on chapters, number of topics is 2:
lda_model_2_topic <- LDA(dtm, k = 2, control = list(seed = 1234))

# Now let's apply LDA on chapters, number of topics is 4:
lda_model_4_topic <- LDA(dtm, k = 4, control = list(seed = 1234))

# Let's extract per-topic-per-word probabilities from the LDA object:
text_2_topics <- tidy(lda_model_2_topic, matrix = "beta")
text_4_topics <- tidy(lda_model_4_topic, matrix = "beta")

# Let's visualize top terms per topic for 2 topic model:

text_2_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()

# Let's visualize top terms per topic for 4 topic model:

text_4_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()


# Now let's see which topics are associated with each document (gamma probabilities):
documents_2_topics <- tidy(lda_model_2_topic, matrix = "gamma")
documents_2_topics

documents_4_topics <- tidy(lda_model_4_topic, matrix = "gamma")
documents_4_topics
```
"Ward No 6" is made up of 26.5% from topic 1, 52.9% from topic 2 and 20.5% from topic 3.
"The lady with the dog" draws entirely from topic 4.
